{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Num:  9\n",
      "total_batch of training data:  720\n",
      "Epoch: 0001 cost= nan\n",
      "Batch Accuracy:  1.0\n",
      "Batch Accuracy:  1.0\n",
      "Batch Accuracy:  1.0\n",
      "Batch Accuracy:  1.0\n",
      "Batch Accuracy:  1.0\n",
      "Batch Accuracy:  1.0\n",
      "Batch Accuracy:  1.0\n",
      "Batch Accuracy:  1.0\n",
      "Batch Accuracy:  1.0\n",
      "Batch Accuracy:  1.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Batch Accuracy:  0.0\n",
      "Final Accuracy:  0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "#author: oger\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import linecache\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "\n",
    "train_file = \"dbpedia.train\"\n",
    "test_file = \"dbpedia.test\"\n",
    "label_dict = {}\n",
    "sku_dict = {}\n",
    "\n",
    "max_window_size = 1000\n",
    "batch_size = 500\n",
    "emb_size = 128\n",
    "\n",
    "# 超参数\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1\n",
    "display_step = 1\n",
    "\n",
    "# 网络参数\n",
    "n_hidden_1 = 128 # 第一层\n",
    "# n_hidden_2 = 256 # 第二层\n",
    "\n",
    "# 初始化数据\n",
    "def init_data(read_file):\n",
    "    #0 is used for padding embedding\n",
    "    label_cnt = 0\n",
    "    sku_cnt = 1\n",
    "    f = open(read_file,'r',encoding='utf-8')\n",
    "    for line in f:\n",
    "        line = line.strip().split(',')\n",
    "        for i in line:\n",
    "            if i[0]==' ':\n",
    "                continue\n",
    "            if i[0] not in label_dict:\n",
    "                label_dict[i[0]] = label_cnt\n",
    "                label_cnt += 1\n",
    "            for j in i[1:]:\n",
    "                if j not in sku_dict:\n",
    "                    sku_dict[j] = sku_cnt\n",
    "                    sku_cnt += 1\n",
    "\n",
    "# 读取数据\n",
    "def read_data(pos, batch_size, data_lst):\n",
    "    batch = data_lst[pos:pos + batch_size]\n",
    "    x = np.zeros((batch_size, max_window_size))\n",
    "    mask = np.zeros((batch_size, max_window_size))\n",
    "    y = []\n",
    "    word_num = np.zeros((batch_size))\n",
    "    line_no = 0\n",
    "    for line in batch:\n",
    "        line = line.strip().split(',')\n",
    "        y.append(label_dict[line[0].strip()])\n",
    "        col_no = 0\n",
    "        for i in line[1:]:\n",
    "            if i in sku_dict:\n",
    "                x[line_no][col_no] = sku_dict[i]\n",
    "                mask[line_no][col_no] = 1\n",
    "                col_no += 1\n",
    "            if col_no >= max_window_size:\n",
    "                break\n",
    "        word_num[line_no] = col_no\n",
    "        line_no += 1\n",
    "\n",
    "    return x, np.array(y).reshape(batch_size, 1), mask.reshape(batch_size, max_window_size, 1), word_num.reshape(batch_size, 1)\n",
    "\n",
    "# 初始化与读取数据\n",
    "init_data(train_file)\n",
    "n_classes = len(label_dict)\n",
    "train_lst = linecache.getlines(train_file)\n",
    "# 类别的个数\n",
    "print(\"Class Num: \", n_classes)\n",
    "\n",
    "# 每一层的w和b\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([emb_size, n_hidden_1])),\n",
    "    # 'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    # 'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# 构建多层感知器\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    #x = tf.nn.dropout(x, 0.8)\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    #dlayer_1 = tf.nn.dropout(layer_1, 0.5)\n",
    "    #layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    #layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    # out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    # return out_layer\n",
    "    return layer_1\n",
    "\n",
    "embedding = {\n",
    "    'input':tf.Variable(tf.random_uniform([len(sku_dict)+1, emb_size], -1.0, 1.0))\n",
    "    # 'output':tf.Variable(tf.random_uniform([len(label_dict)+1, emb_size], -1.0, 1.0))\n",
    "}\n",
    "\n",
    "emb_mask = tf.placeholder(tf.float32, shape=[None, max_window_size, 1])\n",
    "word_num = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "x_batch = tf.placeholder(tf.int32, shape=[None, max_window_size])\n",
    "y_batch = tf.placeholder(tf.int64, [None, 1])\n",
    "\n",
    "input_embedding = tf.nn.embedding_lookup(embedding['input'], x_batch)\n",
    "project_embedding = tf.div(tf.reduce_sum(tf.multiply(input_embedding,emb_mask), 1),word_num)\n",
    "\n",
    "# 多层感知器\n",
    "pred = multilayer_perceptron(project_embedding, weights, biases)\n",
    "\n",
    "# 类似word2vec中的skip-gram中的negative sampling求loss(NCE loss)\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([n_classes, n_hidden_1],\n",
    "                        stddev=1.0 / math.sqrt(n_hidden_1)))\n",
    "nce_biases = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=y_batch,\n",
    "                     inputs=pred,\n",
    "                     num_sampled=9,\n",
    "                     num_classes=n_classes))\n",
    "\n",
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "out_layer = tf.matmul(pred, tf.transpose(nce_weights)) + nce_biases\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # 迭代与训练\n",
    "    start_time = time.time()\n",
    "    total_batch = int(len(train_lst) / batch_size)\n",
    "    print(\"total_batch of training data: \", total_batch)\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        for i in range(total_batch):\n",
    "            x, y, batch_mask, word_number = read_data(i * batch_size, batch_size, train_lst)\n",
    "            _,c = sess.run([optimizer, cost], feed_dict={x_batch: x, emb_mask: batch_mask, word_num: word_number, y_batch: y})\n",
    "            #求平均的loss\n",
    "            avg_cost += c / total_batch\n",
    "            # correct_prediction = tf.equal(tf.argmax(out_layer, 1), tf.reshape(y_batch, [batch_size]))\n",
    "            # accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            # print(\"Accuracy:\", accuracy.eval({x_batch: x, y_batch: y, emb_mask: batch_mask, word_num: word_number}))\n",
    "\n",
    "        # 每个epoch输出信息\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \\\n",
    "              \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # 测试准确率\n",
    "    correct_prediction = tf.equal(tf.argmax(out_layer, 1), tf.reshape(y_batch, [batch_size]))\n",
    "    # 计算准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    test_lst = linecache.getlines(test_file)\n",
    "    total_batch = int(len(test_lst) / batch_size)\n",
    "    final_accuracy = 0\n",
    "    for i in range(total_batch):\n",
    "        x, y, batch_mask, word_number = read_data(i*batch_size, batch_size, test_lst)\n",
    "        batch_accuracy = accuracy.eval({x_batch: x, y_batch: y, emb_mask: batch_mask, word_num: word_number})\n",
    "        print(\"Batch Accuracy: \", batch_accuracy)\n",
    "        final_accuracy += batch_accuracy\n",
    "    print(\"Final Accuracy: \", final_accuracy * 1.0 / total_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>出于运行时间考虑，当前参数下效果不好是正常的，同学们可以自己调一下网络参数和超参数，以提升准确率！</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
